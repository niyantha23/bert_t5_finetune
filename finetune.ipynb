{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import time\n","import datetime\n","import gc\n","import random\n","from nltk.corpus import stopwords\n","import re\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","import transformers\n","from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>review_id</th>\n","      <th>product_id</th>\n","      <th>reviewer_id</th>\n","      <th>review_body</th>\n","      <th>review_title</th>\n","      <th>language</th>\n","      <th>product_category</th>\n","      <th>sentiment_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5000</td>\n","      <td>en_0199937</td>\n","      <td>product_en_0902516</td>\n","      <td>reviewer_en_0097389</td>\n","      <td>These are AWFUL. They are see through, the fab...</td>\n","      <td>Don’t waste your time!</td>\n","      <td>en</td>\n","      <td>apparel</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5001</td>\n","      <td>en_0863335</td>\n","      <td>product_en_0348072</td>\n","      <td>reviewer_en_0601537</td>\n","      <td>I bought 4 and NONE of them worked. Yes I used...</td>\n","      <td>One Star</td>\n","      <td>en</td>\n","      <td>other</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5002</td>\n","      <td>en_0565010</td>\n","      <td>product_en_0356154</td>\n","      <td>reviewer_en_0970351</td>\n","      <td>On first use it didn't heat up and now it does...</td>\n","      <td>Totally useless</td>\n","      <td>en</td>\n","      <td>other</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5003</td>\n","      <td>en_0963290</td>\n","      <td>product_en_0583322</td>\n","      <td>reviewer_en_0216125</td>\n","      <td>You want an HONEST answer? I just returned fro...</td>\n","      <td>Gold filled earrings</td>\n","      <td>en</td>\n","      <td>jewelry</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5004</td>\n","      <td>en_0238156</td>\n","      <td>product_en_0487636</td>\n","      <td>reviewer_en_0514203</td>\n","      <td>The glue works fine but the container is impos...</td>\n","      <td>Poor container</td>\n","      <td>en</td>\n","      <td>industrial_supplies</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0   review_id          product_id          reviewer_id  \\\n","0        5000  en_0199937  product_en_0902516  reviewer_en_0097389   \n","1        5001  en_0863335  product_en_0348072  reviewer_en_0601537   \n","2        5002  en_0565010  product_en_0356154  reviewer_en_0970351   \n","3        5003  en_0963290  product_en_0583322  reviewer_en_0216125   \n","4        5004  en_0238156  product_en_0487636  reviewer_en_0514203   \n","\n","                                         review_body            review_title  \\\n","0  These are AWFUL. They are see through, the fab...  Don’t waste your time!   \n","1  I bought 4 and NONE of them worked. Yes I used...                One Star   \n","2  On first use it didn't heat up and now it does...         Totally useless   \n","3  You want an HONEST answer? I just returned fro...    Gold filled earrings   \n","4  The glue works fine but the container is impos...          Poor container   \n","\n","  language     product_category sentiment_label  \n","0       en              apparel        negative  \n","1       en                other        negative  \n","2       en                other        negative  \n","3       en              jewelry        negative  \n","4       en  industrial_supplies        negative  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"english/english_reviews_test.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["# get the train, val, test splits[x ->y]\n","train = pd.read_csv(\"english/english_reviews_train.csv\")\n","dev=pd.read_csv(\"english/english_reviews_val.csv\")"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["60000"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["len(train)"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["train_reviews=train['review_body']\n","train_labels=train['sentiment_label']\n","dev_reviews=dev['review_body']\n","dev_labels=dev['sentiment_label']"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["encoder=LabelEncoder()\n","train_labels=encoder.fit_transform(train_labels)\n","dev_labels=encoder.fit_transform(dev_labels)"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["max_len=512"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["def tokenize_reviews(reviews, labels, tokenizer, max_len):\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every review...\n","    for review in reviews:\n","        # Tokenize and encode each review\n","        encoded_dict = tokenizer.encode_plus(\n","            review,                      # Review to encode\n","            add_special_tokens=True,     # Add '[CLS]' and '[SEP]'\n","            max_length=max_len,          # Pad & truncate all reviews\n","            pad_to_max_length=True,      # Pad reviews to `max_len`\n","            return_attention_mask=True,  # Construct attention masks\n","            return_tensors='pt',         # Return PyTorch tensors\n","        )\n","        \n","        # Append the encoded input and attention mask to the lists\n","        input_ids.append(encoded_dict['input_ids'])\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert lists to tensors\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    labels = torch.tensor(labels)\n","\n","    return input_ids, attention_masks, labels"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","c:\\Users\\niyan\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2688: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["train_input_ids, train_attention_mask, train_labels=tokenize_reviews(train_reviews,train_labels,tokenizer,max_len)\n","dev_input_ids, dev_attention_mask, dev_labels=tokenize_reviews(dev_reviews,dev_labels,tokenizer,max_len)"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n","dev_dataset=TensorDataset(dev_input_ids, dev_attention_mask, dev_labels)\n"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","val_dataloader = DataLoader(\n","            dev_dataset, # The validation samples.\n","            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 3, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# if device == \"cuda:0\":\n","# # Tell pytorch to run this model on the GPU.\n","#     model = model.cuda()\n","model = model.to(device)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["epochs = 4\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    # Perform one full pass over the training set.\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","    total_train_loss = 0\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the device using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        optimizer.zero_grad()\n","        output = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)        \n","        loss = output.loss\n","        total_train_loss += loss.item()\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","    print(\"\")\n","    print(\"Running Validation...\")\n","    t0 = time.time()\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    best_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    # Evaluate data for one epoch\n","    for batch in val_dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","            output= model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)\n","        loss = output.loss\n","        total_eval_loss += loss.item()\n","        # Move logits and labels to CPU if we are using GPU\n","        logits = output.logits\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(val_dataloader)\n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    if avg_val_accuracy > best_eval_accuracy:\n","        torch.save(model, 'bert_model')\n","        best_eval_accuracy = avg_val_accuracy\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"DL","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":2}
